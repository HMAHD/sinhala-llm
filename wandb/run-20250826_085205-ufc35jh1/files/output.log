

  0%|                                                                                                                                                   | 1/37370 [00:07<79:27:42,  7.66s/it]



































































































  0%|▍                                                                                                                                                | 100/37370 [09:52<61:13:36,  5.91s/it]



































































































  1%|▊                                                                                                                                                | 199/37370 [19:38<61:04:08,  5.91s/it]





































































































  1%|█▏                                                                                                                                               | 300/37370 [29:35<60:51:59,  5.91s/it]




































































































  1%|█▌                                                                                                                                               | 400/37370 [39:27<60:45:24,  5.92s/it]



































































































  1%|█▉                                                                                                                                               | 499/37370 [49:12<60:32:06,  5.91s/it]





































































































  2%|██▎                                                                                                                                              | 600/37370 [59:09<60:22:45,  5.91s/it]




































































































  2%|██▋                                                                                                                                            | 700/37370 [1:09:01<60:13:24,  5.91s/it]



































































































  2%|███                                                                                                                                            | 799/37370 [1:18:46<60:04:21,  5.91s/it]




































































































  2%|███▍                                                                                                                                           | 900/37370 [1:28:44<59:53:53,  5.91s/it]




































































































  3%|███▊                                                                                                                                          | 1000/37370 [1:38:35<59:44:44,  5.91s/it]




































































































  3%|████▏                                                                                                                                         | 1100/37370 [1:48:26<59:31:25,  5.91s/it]



































































































  3%|████▌                                                                                                                                         | 1199/37370 [1:58:12<59:22:16,  5.91s/it]




































































































  3%|████▉                                                                                                                                         | 1299/37370 [2:08:02<59:13:30,  5.91s/it]




































































































  4%|█████▎                                                                                                                                        | 1399/37370 [2:17:53<59:03:20,  5.91s/it]




































































































  4%|█████▋                                                                                                                                        | 1499/37370 [2:27:44<58:51:17,  5.91s/it]




































































































  4%|██████                                                                                                                                        | 1599/37370 [2:37:35<58:43:33,  5.91s/it]



































































































  5%|██████▍                                                                                                                                       | 1699/37370 [2:47:26<58:36:34,  5.92s/it]





































































































  5%|██████▊                                                                                                                                       | 1800/37370 [2:57:24<58:25:30,  5.91s/it]



































































































  5%|███████▏                                                                                                                                      | 1899/37370 [3:07:10<58:18:31,  5.92s/it]



































































































  5%|███████▌                                                                                                                                      | 2000/37370 [3:17:07<58:10:10,  5.92s/it]



































































































  6%|███████▉                                                                                                                                      | 2099/37370 [3:26:54<58:01:54,  5.92s/it]





































































































  6%|████████▎                                                                                                                                     | 2200/37370 [3:36:51<57:46:23,  5.91s/it]



































































































  6%|████████▋                                                                                                                                     | 2299/37370 [3:46:37<57:38:27,  5.92s/it]





































































































  6%|█████████                                                                                                                                     | 2400/37370 [3:56:34<57:27:35,  5.92s/it]
































































  7%|█████████▎                                                                                                                                    | 2464/37370 [4:03:17<65:01:00,  6.71s/it]ERROR:__main__:Training failed: recompile_limit reached with one_graph=True. Excessive recompilations can degrade performance due to the compilation overhead of each recompilation. To monitor recompilations, enable TORCH_LOGS=recompiles. If recompilations are expected, consider increasing torch._dynamo.config.cache_size_limit to an appropriate value.
Traceback (most recent call last):
  File "/workspace/sriai/train.py", line 431, in <module>
    trainer.run()
  File "/workspace/sriai/train.py", line 407, in run
    self.train()
  File "/workspace/sriai/train.py", line 362, in train
    result = self.trainer.train()  # No resume - starting fresh
  File "/workspace/sriai/unsloth_compiled_cache/UnslothSFTTrainer.py", line 738, in train
    output = super().train(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "<string>", line 317, in _fast_inner_training_loop
  File "<string>", line 31, in _unsloth_training_step
  File "/usr/local/lib/python3.10/dist-packages/unsloth/models/_utils.py", line 1243, in _unsloth_pre_compute_loss
    outputs = self._old_compute_loss(model, inputs, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 3801, in compute_loss
    outputs = model(**inputs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 820, in forward
    return model_forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 53, in inner
    return disable_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py", line 1372, in PeftModel_fast_forward
    return self.base_model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py", line 1248, in _CausalLM_fast_forward
    loss = unsloth_fused_ce_loss(
  File "/usr/local/lib/python3.10/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py", line 352, in unsloth_fused_ce_loss
    return apply_autograd_function(UnslothFusedLoss, dict(
  File "/usr/local/lib/python3.10/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py", line 40, in apply_autograd_function
    return getattr(autograd, "apply")(*(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 576, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py", line 292, in forward
    accumulate_chunk(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
    return self._torchdynamo_orig_callable(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1033, in _compile
    raise FailOnRecompileLimitHit(
torch._dynamo.exc.FailOnRecompileLimitHit: recompile_limit reached with one_graph=True. Excessive recompilations can degrade performance due to the compilation overhead of each recompilation. To monitor recompilations, enable TORCH_LOGS=recompiles. If recompilations are expected, consider increasing torch._dynamo.config.cache_size_limit to an appropriate value.
Traceback (most recent call last):
  File "/workspace/sriai/train.py", line 431, in <module>
    trainer.run()
  File "/workspace/sriai/train.py", line 407, in run
    self.train()
  File "/workspace/sriai/train.py", line 362, in train
    result = self.trainer.train()  # No resume - starting fresh
  File "/workspace/sriai/unsloth_compiled_cache/UnslothSFTTrainer.py", line 738, in train
    output = super().train(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "<string>", line 317, in _fast_inner_training_loop
  File "<string>", line 31, in _unsloth_training_step
  File "/usr/local/lib/python3.10/dist-packages/unsloth/models/_utils.py", line 1243, in _unsloth_pre_compute_loss
    outputs = self._old_compute_loss(model, inputs, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py", line 3801, in compute_loss
    outputs = model(**inputs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 820, in forward
    return model_forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_compile.py", line 53, in inner
    return disable_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py", line 1372, in PeftModel_fast_forward
    return self.base_model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py", line 1248, in _CausalLM_fast_forward
    loss = unsloth_fused_ce_loss(
  File "/usr/local/lib/python3.10/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py", line 352, in unsloth_fused_ce_loss
    return apply_autograd_function(UnslothFusedLoss, dict(
  File "/usr/local/lib/python3.10/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py", line 40, in apply_autograd_function
    return getattr(autograd, "apply")(*(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py", line 576, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/usr/local/lib/python3.10/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py", line 292, in forward
    accumulate_chunk(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
    return self._torchdynamo_orig_callable(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
    return _compile(
  File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py", line 1033, in _compile
    raise FailOnRecompileLimitHit(
torch._dynamo.exc.FailOnRecompileLimitHit: recompile_limit reached with one_graph=True. Excessive recompilations can degrade performance due to the compilation overhead of each recompilation. To monitor recompilations, enable TORCH_LOGS=recompiles. If recompilations are expected, consider increasing torch._dynamo.config.cache_size_limit to an appropriate value.